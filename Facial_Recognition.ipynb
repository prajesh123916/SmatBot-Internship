{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1290d2cc",
      "metadata": {
        "id": "1290d2cc"
      },
      "source": [
        "1 Face Recognition using CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dbabe6e",
      "metadata": {
        "id": "3dbabe6e"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import TensorBoard\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import np_utils\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e8facfa",
      "metadata": {
        "id": "6e8facfa"
      },
      "source": [
        "### 2 Step2:\n",
        "• Load Dataset :\n",
        "\n",
        "After loading the Dataset we have to normalize every image.\n",
        "Note: an image is a Uint8 matrix of pixels and for calculation, you need to convert the format of\n",
        "the image to float or double"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d04003c",
      "metadata": {
        "id": "8d04003c"
      },
      "outputs": [],
      "source": [
        "#load dataset\n",
        "data = np.load('ORL_faces.npz')\n",
        "# load the \"Train Images\"\n",
        "x_train = data['trainX']\n",
        "#normalize every image\n",
        "x_train = np.array(x_train,dtype='float32')/255\n",
        "x_test = data['testX']\n",
        "x_test = np.array(x_test,dtype='float32')/255\n",
        "# load the Label of Images\n",
        "y_train= data['trainY']\n",
        "y_test= data['testY']\n",
        "# show the train and test Data format\n",
        "print('x_train : {}'.format(x_train[:]))\n",
        "print('Y-train shape: {}'.format(y_train))\n",
        "print('x_test shape: {}'.format(x_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ff51a9",
      "metadata": {
        "id": "53ff51a9"
      },
      "source": [
        "### 3 Step 3\n",
        "Split DataSet : Validation data and Train\n",
        "\n",
        "Validation DataSet: this data set is used to minimize overfitting.If the accuracy over the training\n",
        "data set increases, but the accuracy over then validation data set stays the same or decreases, then\n",
        "we’re overfitting your neural network and you should stop training.\n",
        "\n",
        "• Note: we usually use 30 percent of every dataset as the validation data but Here we only used\n",
        "5 percent because the number of images in this dataset is very low."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26d4231b",
      "metadata": {
        "id": "26d4231b"
      },
      "outputs": [],
      "source": [
        "x_train, x_valid, y_train, y_valid= train_test_split(\n",
        "x_train, y_train, test_size=.05, random_state=1234,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "386c2aae",
      "metadata": {
        "id": "386c2aae"
      },
      "source": [
        "### 4 Step 4\n",
        "for using the CNN, we need to change The size of images ( The size of images must be the same)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4cfb38",
      "metadata": {
        "id": "8d4cfb38"
      },
      "outputs": [],
      "source": [
        "im_rows=112\n",
        "im_cols=92\n",
        "batch_size=512\n",
        "im_shape=(im_rows, im_cols, 1)\n",
        "#change the size of images\n",
        "x_train = x_train.reshape(x_train.shape[0], *im_shape)\n",
        "x_test = x_test.reshape(x_test.shape[0], *im_shape)\n",
        "x_valid = x_valid.reshape(x_valid.shape[0], *im_shape)\n",
        "print('x_train shape: {}'.format(y_train.shape[0]))\n",
        "print('x_test shape: {}'.format(y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dcab837",
      "metadata": {
        "id": "3dcab837"
      },
      "source": [
        "### 5 Step 5\n",
        "Build CNN model: CNN have 3 main layer: * 1-Convolotional layer * 2- pooling layer\n",
        "* 3- fully connected layer\n",
        "we could build a new architecture of CNN by changing the number and position of layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8a5361a",
      "metadata": {
        "id": "b8a5361a"
      },
      "outputs": [],
      "source": [
        "#filters= the depth of output image or kernels\n",
        "cnn_model= Sequential([\n",
        "Conv2D(filters=36, kernel_size=7, activation='relu', input_shape= im_shape),\n",
        "MaxPooling2D(pool_size=2),\n",
        "Conv2D(filters=54, kernel_size=5, activation='relu', input_shape= im_shape),\n",
        "MaxPooling2D(pool_size=2),\n",
        "Flatten(),\n",
        "Dense(2024, activation='relu'),\n",
        "Dropout(0.5),\n",
        "Dense(1024, activation='relu'),\n",
        "Dropout(0.5),\n",
        "Dense(512, activation='relu'),\n",
        "Dropout(0.5),\n",
        "#20 is the number of outputs\n",
        "3\n",
        "Dense(20, activation='softmax')\n",
        "])\n",
        "cnn_model.compile(\n",
        "loss='sparse_categorical_crossentropy',#'categorical_crossentropy',\n",
        "optimizer=Adam(lr=0.0001),\n",
        "metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd370072",
      "metadata": {
        "id": "bd370072"
      },
      "source": [
        "### 6 Step 6\n",
        "Train the Model\n",
        "• Note: We can change the number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3449065f",
      "metadata": {
        "id": "3449065f"
      },
      "outputs": [],
      "source": [
        ": history=cnn_model.fit(\n",
        "np.array(x_train), np.array(y_train), batch_size=512,\n",
        "epochs=250, verbose=2,\n",
        "validation_data=(np.array(x_valid),np.array(y_valid)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100afb61",
      "metadata": {
        "id": "100afb61"
      },
      "outputs": [],
      "source": [
        "scor = cnn_model.evaluate( np.array(x_test), np.array(y_test), verbose=0)\n",
        "print('test los {:.4f}'.format(scor[0]))\n",
        "print('test acc {:.4f}'.format(scor[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e770cc",
      "metadata": {
        "id": "86e770cc"
      },
      "source": [
        "### 7 Step 7\n",
        "plot the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee62024",
      "metadata": {
        "id": "dee62024"
      },
      "outputs": [],
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1758e2c",
      "metadata": {
        "id": "f1758e2c"
      },
      "source": [
        "### 8 step 8\n",
        "Plot Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacd7058",
      "metadata": {
        "id": "cacd7058"
      },
      "outputs": [],
      "source": [
        "predicted =np.array( cnn_model.predict(x_test))\n",
        "#print(predicted)\n",
        "#print(y_test)\n",
        "ynew = cnn_model.predict_classes(x_test)\n",
        "Acc=accuracy_score(y_test, ynew)\n",
        "print(\"accuracy : \")\n",
        "print(Acc)\n",
        "#/tn, fp, fn, tp = confusion_matrix(np.array(y_test), ynew).ravel()\n",
        "cnf_matrix=confusion_matrix(np.array(y_test), ynew)\n",
        "y_test1 = np_utils.to_categorical(y_test, 20)\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "normalize=False,\n",
        "17\n",
        "title='Confusion matrix',\n",
        "cmap=plt.cm.Blues):\n",
        "\"\"\"\n",
        "This function prints and plots the confusion matrix.\n",
        "Normalization can be applied by setting `normalize=True`.\n",
        "\"\"\"\n",
        "if normalize:\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "#print(\"Normalized confusion matrix\")\n",
        "else:\n",
        "print('Confusion matrix, without normalization')\n",
        "#print(cm)\n",
        "plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "plt.title(title)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=45)\n",
        "plt.yticks(tick_marks, classes)\n",
        "fmt = '.2f' if normalize else 'd'\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "plt.text(j, i, format(cm[i, j], fmt),\n",
        "horizontalalignment=\"center\",\n",
        "color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n",
        "print('Confusion matrix, without normalization')\n",
        "print(cnf_matrix)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix[1:10,1:10], classes=[0,1,2,3,4,5,6,7,8,9],\n",
        "title='Confusion matrix, without normalization')\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix[11:20,11:20],␣\n",
        ",→classes=[10,11,12,13,14,15,16,17,18,19],\n",
        "title='Confusion matrix, without normalization')\n",
        "print(\"Confusion matrix:\\n%s\" % confusion_matrix(np.array(y_test), ynew))\n",
        "print(classification_report(np.array(y_test), ynew))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}